\chapter{Теоретическая часть}

\section{Формализация алгоритмов оптимизации}

При реализации необходимых алгоритмов под заданные требования интерфейсов были выбраны следующие методы минимизации: 

\begin{itemize}
	\item универсальный -- алгоритм имитации отжига,
	\item требующий \texttt{IDifferentiableFunctional} -- метод сопряжённых градиентов,
	\item требующий \texttt{ILeastSquaresFunctional} -- алгоритм Левенберга-Марквардта, производный от Гаусса-Ньютона.
\end{itemize}

Формализация данных алгоритмов приведена ниже.

\subsection{Алгоритм имитации отжига}

\begin{enumerate}
	\item Инициализируем:
	\begin{itemize}
		\item начальный вектор параметров $p_0$ -- состояние системы;
		\item оператор $A(p, i)$ -- случайно генерирующий новое состояние системы после i-ого шага с учётом текущего состояния $p$. 
		\item Правила расчёта вероятности перехода в новое состояние: симуляция отжига (\texttt{AnnealingSimulation}), а также последовательное (\texttt{ContinuousImprovement}) или пороговое (\texttt{ThresholdImprovement}) улучшения;
		\item $T_i > 0$ — убывающей к нулю положительной последовательности. Закон убывания также может задаваться произвольно, например отжигом Бальцмана (\texttt{BalzmanAnnealing}) или Коши (\texttt{BasketFiring}).
	\end{itemize}
	\item К точке $x_{i}$ применяется оператор $A$, в результате чего получается $x_{i}^{*}=A(x_{i},i)$, для которой вычисляется $\Delta F_{i}=F({x_{i}^{*}})-F({x_{i}})$
	\item Если ($\Delta F_{i}\leq 0$), то $x_{i+1}={x_{i}^{*}}$. Иначе ($\Delta F_{i}>0$), переход в новое состояние осуществляется с некоторой вероятностью, зависящей от величины повышения энергии и текущей температуры, в соответствии с законом перехода состояния.
	\item Если переход не произошёл, состояние системы остаётся прежним: $x_{i+1}={x_{i}}$ и переходим в п.2. 
	\item Алгоритм останавливается по достижении точки, которая оказывается при температуре близкой к нулю.
\end{enumerate}

\subsection{Метод сопряжённых градиентов}

\begin{enumerate}
	\item Задаются начальное приближение и погрешность: $\vec {p}_{0}, \varepsilon, k=0$
	\item Рассчитывается начальное направление: $j=0, {\vec {S}}_{k}^{j}=-\nabla f({\vec {p}}_{k}), {\vec {p}}_{k}^{j}={\vec {p}}_{k}$
	\item Находится вектор следующего приближения: ${\vec {p}}_{k}^{j+1}={\vec {p}}_{k}^{j}+\lambda {\vec {S}}_{p}^{j},$ 
	\item Производится расчёт вспомогательных параметров и вектора:
	\begin{itemize}
		\item $\lambda =\arg \min _{\lambda }f({\vec {p}}_{k}^{j}+\lambda {\vec {S}}_{k}^{j}),$
		\item $\omega ={\frac {||\nabla f({\vec {p}}_{k}^{j+1})||^{2}}{||\nabla f({\vec {p}}_{k}^{j})||^{2}}}$
		\item ${\vec {S}}_{k}^{j+1}=-\nabla f({\vec {p}}_{k}^{j+1})+\omega {\vec {S}}_{k}^{j},$
	\end{itemize}
	
	\item Если $||{\vec {S}}_{k}^{j+1}||<\varepsilon $ или $\displaystyle ||{\vec {p}}_{k}^{j+1}-{\vec {p}}_{k}^{j}||<\varepsilon $, то ${\vec {p}}={\vec {p}}_{k}^{j+1}$ и остановка.
	Иначе
	\begin{itemize}
		\item если $(j+1)<n$ $\displaystyle j=j+1$ и переход к 3
		\item иначе ${\vec {p}}_{k+1}={\vec {p}}_{k}^{j+1},\quad k=k+1$ и переход к 2.
	\end{itemize}
\end{enumerate}

\subsection{Алгоритм Левенберга-Марквардта}

\begin{enumerate}
	\item Определяется направление поиска Левенберга — Марквардта  из решения СЛАУ: 
	$$[J^{T}({\vec {p}}_{k})J({\vec {p}}_{k})+\lambda _{k}I]{\vec {\Delta p}}_{k}=-J^{T}({\vec {p}}_{k}){\vec {f}}({\vec {p}}_{k}),$$
	где $\lambda _{k}$ — некоторая неотрицательная константа, определяемая на каждом шаге, $I$ — единичная матрица, $J({\vec {p}})$ — матрица Якоби вектор-функции ${\vec {f}}({\vec {p}})$. 
	\item Находим вектор следующего приближения: $\vec {p}_{k+1}={\vec {p}}_{k}+{\Delta \vec {p}}_{k}.$
	\item Параметр $\lambda _{k}$  увеличивать до тех пор, пока не будет достигнуто условие $F({\vec {p}}_{k+1})<F({\vec {p}}_{k})$ или его можно устанавливать исходя из отношения между фактическими изменениями функции ${\vec {f}}({\vec {p}}),$ достигнутыми в результате пробных шагов, и ожидаемыми величинами этих изменений при интерполяции.
	\item Процедура прекращается при выполнении одного из условий: $|| \grad F(\overrightarrow{p})|| < \varepsilon_1$ или $|| \Delta \overrightarrow{p}|| < \varepsilon_2$.
\end{enumerate}



\section{Представление функций в программе}

При реализации функций в основе лежали следующие идеи их представления.

\begin{itemize}
	\item \textsl{Линейная функция в n-мерном пространстве} будет искаться в виде $f(\overline{x}) = c_0 + c_1 \cdot x_1 + c_2 \cdot x_2 + ... + c_n \cdot x_n$. Вектор входных параметров для данной функции выглядит следующим образом: [$c_0, c_1, ..., c_n$].
	\item \textsl{Полином n-й степени в одномерном пространстве} будет искаться в виде $P(x) = c_0 + c_1 \cdot x + c_2 \cdot x^2 + ... + c_n \cdot x^n$. Вектор входных параметров для данной функции выглядит следующим образом: [$c_0, c_1, ..., c_n$].
	\item \textsl{Кусочно-линейную функции} представим в виде $f(x) = ax + b + c_1 \abs{x - x_1} + c_2 \abs{x - x_2} + ... + c_n \abs{x - x_n}$. Тогда вектор входных параметров будет иметь вид: $[x_0, x_1, ... x_n, a, b, c_0, c_1, ... c_n]$, где $[x_0, x_1, ... x_n]$ - координаты точек разлома, $a$ - задаёт наклон основной линейной части графика, $b$ - свободный член, определяющий вертикальный сдвиг функции, $[c_0, c_1, ... c_n]$ - коэффициенты, управляющие влиянием разрыва наклона (излома).
	\item \textsl{Сглаживающий сплайн} вида $S(x) = \sum_{i = 0}^{2n} q_i \cdot \psi_i(x)$ на эрмитовых базисных функциях. Вектор входных параметров определяется видом: $[q_0, ..., q_{2n}, x_0, ..., x_n]$.
\end{itemize}